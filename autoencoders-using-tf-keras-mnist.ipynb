{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"# Autoencoder\nAutoencoders are used to learn efficient data codings in an unsupervised manner. The aim is to learn a representation (encoding) for a set of data, typically for the purpose of dimentionality reduction. The concept has become more widely used for generative models of data. Some of most powerful algorithms recently have involved sparse autoencoders stacked inside of deep neural networks.\n\nWe will try to create following autoencoders:\n- a simple autoencoder based on a fully connected layer\n- a sparse autoencoder\n- a deep fully-connected autoencoder\n- a deep convolutional autoencoder\n- an image denoising model\n- a sequence-to-sequence autoencoder\n- a variational autoenoder"},{"metadata":{"_uuid":"2b7bf4f9be4c53b593d41abadbc2325f4f776ecf"},"cell_type":"markdown","source":"Autoencoders are data compression algorithms which are data-specific, lossy and learned automatically from examples. The compression and depcompression functions are implemented with neural networks.\n- Data specific: They will only be able to compress data similar to what they have been trained on. Unlike normal compression algorithms, they are not very generic about the data they hold. For example, an autoencoder trained on pictures of faces would do a poor job of compressing pictures of trees because the features it learned are face-specific.\n- Lossy: The decompressed outputs will be degraded compared to original inputs\n- Auto learned: It's easy to train specialized instances of the algorithm that will perform well on a specific type of input. It doesn't require new engineering, just appropriate training data\n\nWe need 3 things to build autoencodere: an encoding function, a decoding function and a distance function between the amount of information loss between the compressed representation of data and the decompressed representation (loss function).\n\n**Are they good at data compression?**\nNot really. The fact that they are are data-specific makes the generally impractival for real-world data compression problems. Making them genreal requires a lot of training data. Future advances might change this.\n\n**What are they good for?**\nData denoising and dimensionality reduction for data visualization. With appropriate dimensionality and sparsity constraints, autoencoders can learn data projections that are more interesting than PCA or other basic techniques.\n\n**What's the big deal with autoencoders?**\nAbsolutely fascinating for newcomers. They have long beein throught to be a potential avenue for the learning of useful representations without the need for labels. But since they are self-supervised, it's not a true unsupervised learning technique. "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7a9b9c36a12ae90bc632236dc43819652fbb0567"},"cell_type":"code","source":"def load_data(path):\n    with np.load(path) as f:\n        x_train, y_train = f['x_train'], f['y_train']\n        x_test, y_test = f['x_test'], f['y_test']\n        return (x_train, y_train), (x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6349e4954e84e598875d7ebf0da514ea33a5cf07","collapsed":true},"cell_type":"code","source":"# we will start simple with a single fully-connected neural layer as encoder and decoder\n# this is the siez of our encoded representations\nENCODING_DIM = 32\n\n# input placeholder\ninput_img = tf.keras.layers.Input(shape=(784,))\n\n# this is the encoded representation of the input\nencoded = tf.keras.layers.Dense(ENCODING_DIM, activation='relu')(input_img)\n\n# this is the loss reconstruction of the input\ndecoded = tf.keras.layers.Dense(784, activation='sigmoid')(encoded)\n\n# this model maps an input to its recommendation\nautoencoder = tf.keras.models.Model(input_img, decoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ed0ba68f0901d990696c2c99d8ae79573062903","collapsed":true},"cell_type":"code","source":"# let's also create a seprate encoder model\n# this mode maps an input to its encoded representation\nencoder = tf.keras.models.Model(input_img, encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51765643141c3d75a230aa546987ce1056c045e1","collapsed":true},"cell_type":"code","source":"# as well as decoder model\n# create a placeholder for an encoded (32-dimensional) input\nencoded_input = tf.keras.layers.Input(shape=(ENCODING_DIM,))\n\n# retrieve the last layer of the autoencoder model\ndecoder_layer = autoencoder.layers[-1]\n\n# create the decoder model\ndecoder = tf.keras.models.Model(encoded_input, decoder_layer(encoded_input))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1ef1d7cb58713f533adb07c9f6ea95977ed44b2c"},"cell_type":"code","source":"# Now let's train our autoencoder to reconstruct MNIST digits\n# first we will configure our model to use a per-pixel binary crossentropy loss, and the Adadelta optimizer\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f186a9ad106a788fa9f3d3bb9d0120fecaf280d2","collapsed":true},"cell_type":"code","source":"# let's prepare our input data. We are using MNIST digits and we are disregrading the labels (since we are only interested in encoding/decoding the input images)\n# load the data\n(x_train, _), (x_test, _) = load_data('../input/mnist.npz')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d130b8504a6288baea58220865a66451d976160e"},"cell_type":"code","source":"x_train = x_train.astype('float32') / 255\nx_test = x_test.astype('float32') / 255\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\nprint(x_train.shape)\nprint(x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6421f91efca129a800ecc924b9785ac9877a90d5","_kg_hide-output":true},"cell_type":"code","source":"# now let's train our autoencoder for 50 epochs\nautoencoder.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=True, validation_data=(x_test, x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"116d321f464d9206b6eb350f2865359ae4dbd3d3","collapsed":true},"cell_type":"code","source":"# after 50 epochs the autoencoder seems to reach a stable train/test loss value of about 0.11. We can try to visualize the reconstructed inputs and the encoded representations. We will be using Matplotlib\n# encode and decode some digits\n# note that we take them from the \"test\" set\nencoded_imgs = encoder.predict(x_test)\ndecoded_imgs = decoder.predict(encoded_imgs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14858a1264fd5219566026623d8f06f98a759ca6"},"cell_type":"code","source":"# now using Matplotlib to plot the images\nn = 10 # how many images we will display\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # display original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    \n    # display reconstruction\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cedde6daf149cc161cc17ceb935c8f8c24066887"},"cell_type":"markdown","source":"Adding a sparsity constraint on the encoded representations\nThe representations were only constrained by the size of the hidden layer (32). In such a situation, the hidden layer is learning an approximation of PCA. But another way to constain the representation to be compact is to add a sparsity constraint on the activity of the hiddne representations, so fewer units would \"fire\" at a given time. We can do this by adding `activity_regularizer` to our `Dense` layer."},{"metadata":{"trusted":true,"_uuid":"d41c232133bd0be91441df9b69d7a516caa9d036","_kg_hide-output":true},"cell_type":"code","source":"ENCODING_DIM = 32\n\ninput_img = tf.keras.layers.Input(shape=(784,))\n\n# add a dense layer with L1 activity regularizer\nencoded = tf.keras.layers.Dense(ENCODING_DIM, activation='relu', activity_regularizer=tf.keras.regularizers.l1(10e-5))(input_img)\ndecoded = tf.keras.layers.Dense(784, activation='sigmoid')(encoded)\nautoencoder = tf.keras.models.Model(input_img, decoded)\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n\n# now let's train this for 100 epochs (with added regularization, the model is less likely to overfit and can be trained longer). The model ends with a train loss of 0.11 and test loss of 0.10. The difference is mostly due to the regularization term being added to the loss during training\nautoencoder.fit(x_train, x_train, epochs=100, batch_size=256, shuffle=True, validation_data=(x_test, x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dab629feb565109e32866e2cf1c01bb07569e9cf"},"cell_type":"code","source":"# now using Matplotlib to plot the images\nn = 10 # how many images we will display\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # display original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    \n    # display reconstruction\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c75540b69ead49ba2170aafd1c8cc557125845ed"},"cell_type":"markdown","source":"The images look pretty similar to the previous model, the only significant difference being the sparsity of the encoded representations. `encoded_imgs.mean()` yields a value of 3.33 over 10,000 test images. whereas with previous model the same quantity was 7.30. So our new model yields encoded representations that are twice sparser.\n\n**Deep autoencoder**\nWe do not have to limit ourselves to a single layer as encoder or decoder, we could instead use a stack of layers."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"174cf23abc172e65c274ff4df5c7c557d7a31ced"},"cell_type":"code","source":"input_img = tf.keras.layers.Input(shape=(784,))\nencoded = tf.keras.layers.Dense(128, activation='relu')(input_img)\nencoded = tf.keras.layers.Dense(64, activation='relu')(encoded)\nencoded = tf.keras.layers.Dense(32, activation='relu')(encoded)\n\ndecoded = tf.keras.layers.Dense(64, activation='relu')(encoded)\ndecoded = tf.keras.layers.Dense(128, activation='relu')(decoded)\ndecoded = tf.keras.layers.Dense(784, activation='sigmoid')(decoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"996a5443874301ea1ae2e5acf16288f75a053b53","_kg_hide-output":true},"cell_type":"code","source":"# let's try this\nautoencoder = tf.keras.models.Model(input_img, decoded)\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n\nautoencoder.fit(x_train, x_train, epochs=100, batch_size=256, shuffle=True, validation_data=(x_test, x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b33d8f20b38e69a7202caeac158b11301de51490"},"cell_type":"code","source":"# now using Matplotlib to plot the images\nn = 10 # how many images we will display\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # display original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    \n    # display reconstruction\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e312587cf9e63b216977e0ee6b67f9fd8a88e95"},"cell_type":"markdown","source":"Convolutional autoencoder\n\nSince our inputs are images, it makes sense to use convolutional neural networks (CNNs) as encoders and decoders. In practical settings, autoencoders for images are always convolutional since they perform much better.\n\nThis encoder will consist in a stack of `Conv2D` and `MaxPooling2D` layers (for spatial down-sampling), while the decoder will consist in a stack of `Conv2D` and `UpSampling2D` layers."},{"metadata":{"trusted":true,"_uuid":"a1018831fdbbfb850efc7f37fcc58bc452d2e03b","collapsed":true},"cell_type":"code","source":"input_img = tf.keras.layers.Input(shape=(28, 28, 1)) # adapt this if using `channels_first` image data format\n\nx = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\nx = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\nx = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\nx = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nencoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n\n# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n\nx = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\nx = tf.keras.layers.UpSampling2D((2, 2))(x)\nx = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = tf.keras.layers.UpSampling2D((2, 2))(x)\nx = tf.keras.layers.Conv2D(16, (3, 3), activation='relu')(x)\nx = tf.keras.layers.UpSampling2D((2, 2))(x)\ndecoded = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n\nautoencoder = tf.keras.models.Model(input_img, decoded)\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a6028dbec49a653296553602adbe64a3d31d6ed9"},"cell_type":"code","source":"# to train this model we will with original MNIST digits with shape (samples, 3, 28, 28) and we will just normalize pixel values between 0 and 1\n(x_train, _), (x_test, _) = load_data('../input/mnist.npz')\n\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\nx_test = np.reshape(x_test, (len(x_test), 28, 28, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"485bb23ee67b2ead1f1ec454326a1cabc995eff1","_kg_hide-output":true},"cell_type":"code","source":"autoencoder.fit(x_train, x_train, epochs=50, batch_size=128, shuffle=True, validation_data=(x_test, x_test), callbacks=[tf.keras.callbacks.TensorBoard(log_dir='./tmp/autoencoder')])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29103c9e23d1f1ba10874bb05dcd8e89c8075f1f"},"cell_type":"markdown","source":"The model converges to a loss of 0.098, significantly better than previous model which is in large part due to the higher entropic capacity of the encoded representations (128 vs 32 previously)."},{"metadata":{"trusted":true,"_uuid":"555579d1cb40a706f455307cd4feff5d506b9c40"},"cell_type":"code","source":"decoded_imgs = autoencoder.predict(x_test)\n\nn = 10\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # display original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # display reconstruction\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae79608359a628aa2b8a351f580b12734e54b21b","collapsed":true},"cell_type":"code","source":"# # we can also look at the 128-dimensional encoded representations. These representations are 8x4x4, so we reshape them to 4x32 in order to be able to display them as grayscale images\n# n = 10\n# plt.figure(figsize=(20, 8))\n# for i in range(n):\n#     ax = plt.subplot(1, n, i + 1)\n#     plt.imshow(encoded_imgs[i + 1].reshape(4, 4 * 8).T)\n#     plt.gray()\n#     ax.get_xaxis().set_visible(False)\n#     ax.get_yaxis().set_visible(False)\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eff13363f58882431933fc230ac7d7e2f7d631ad"},"cell_type":"markdown","source":"Application to image denoising\n\nLet's put our convolutional autoencoder to work on an image denoising problem. We will train the autoencoder to map noisy digits images to clean digit images. We will generate synthetic noisy digits by applying a gaussian noise matrix and clip the images between 0 and 1."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f551217d3a192881b862c249158ecfadd375056e"},"cell_type":"code","source":"(x_train, _), (x_test, _) = load_data('../input/mnist.npz')\n\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\nx_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n\nnoise_factor = 0.5\nx_train_noisy = x_train + noise_factor + np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\nx_test_noisy = x_test + noise_factor + np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d66b7e2298faf79b252a993cbe3e8ac6a5ff359"},"cell_type":"code","source":"# here's what the noisy digits look like\nn = 10\nplt.figure(figsize=(20, 2))\nfor i in range(n):\n    ax = plt.subplot(1, n, i + 1)\n    plt.imshow(x_test_noisy[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50876255cff1014e168a486281dc589480f1e36a","collapsed":true},"cell_type":"code","source":"# Can our autoencoder learn to recover the original digits. We will use a slightly different model with more filters per layer\ninput_img = tf.keras.layers.Input(shape=(28, 28, 1))\n\nx = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\nx = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\nx = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\nencoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n\n# the representation is (7, 7, 32)\n\nx = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\nx = tf.keras.layers.UpSampling2D((2, 2))(x)\nx = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\nx = tf.keras.layers.UpSampling2D((2, 2))(x)\ndecoded = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n\nautoencoder = tf.keras.models.Model(input_img, decoded)\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"be79bebe3aecc00a3f9fdd8e2da0f650efcd60df","_kg_hide-output":true},"cell_type":"code","source":"# let's train for 100 epochs\nautoencoder.fit(x_train_noisy, x_train, epochs=100, batch_size=128, shuffle=True, validation_data=(x_test_noisy, x_test), callbacks=[tf.keras.callbacks.TensorBoard(log_dir='./tmp/tb', histogram_freq=0, write_graph=False)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe992cc47326291b7f2b41ae0ddbbe4212908fd4"},"cell_type":"code","source":"# Let's see how it did\ndecoded_imgs = autoencoder.predict(x_test)\n\nn = 10\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # display original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(x_test_noisy[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # display reconstruction\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a091f71a78e216c3ebed64bdb9f05cbeb14a108c"},"cell_type":"markdown","source":"Sequence-to-sequence autoencoder\n\nIf your inputs are sequences, rather than vectors of 2D images, then you may want your encoder and decoder to capture temporal structure, such as a LSTM. To build a LSTM-based autoencoder, first use a LSTM encoder to turn your input sequences into a single vecotr that contains information about the entire sequence, then repeat this vector n times (where n is the number of timesteps in the output sequence), and run a LSTM decoder to turn this constant sequence into the target sequence.\n\nFollowing is a code example which you can implement for your dataset."},{"metadata":{"trusted":true,"_uuid":"e329640ea1a99ee345f355405234f50d6be0796c","collapsed":true},"cell_type":"code","source":"# inputs = tf.keras.layers.Input(shape=(timesteps, input_dim))\n# encoded = tf.keras.layers.LSTM(latent_dim)(inputs)\n\n# decoded = tf.keras.layers.RepeatVector(timesteps)(encoded)\n# decoded = tf.keras.layers.LSTM(input_dmi, return_sequences=True)(decoded)\n\n# sequence_autoencoder = tf.keras.models.Model(inputs, decoded)\n# encoder = tf.keras.models.Model(inputs, encoded)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4576c65796c42ab9030409f21b13834b7e96b027"},"cell_type":"markdown","source":"Variational Autoencoder (VAE)\n\nVariational autoencoders are a slightly more modern and interesting take on autoencoding. It's a type of autoencoder with added constraints on the encoded representations being learned. More precisely, it is an autoencoder that learns a latent variable model for its input data. So instead of letting your neural network learn an arbitrary function, you are learning the parameters of a probability distribution modeling your data. If you sample points from this distribution, you an generate new input data samples. VAE is a generative model.\n\n**How does it work?**\nFirst, an encoder network turns the input samples `x` into two parameters in a latent space, which we will note as `z_mean` and `z_log_sigma`. Then we randomly sample similar points `z` from the latent normal distribution that is assumed to generate the data, via `z = z_mean + exp(x_log_sigma) * epsilon`, where `epsilon` is a random normal tensor. Finally, a deocded network maps these latent space points back to the original input data.\n\nThe parameters of the model are trained via two loss functions: a reconstriction loss forcing the decoded samples to match the initial inputs (just like in our previous autoencoders), and the KL divergence between the learned latent distribution and the prior distribution, acting as a regulaization term. You could actually get rid of this latter term entirely, although it does help in learning well-formed latent spaces and reducing overfitting to the training data."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7033562b28b56d02a768a92b0c7e2aa705f8daf8"},"cell_type":"code","source":"# helper functions\n\n# reparameterization trick\n# instead of sampling from Q(z|X), sample eps = N(0,I)\n# z = z_mean + sqrt(var)*eps\ndef sampling(args):\n    z_mean, z_log_var = args\n    batch = tf.keras.backend.shape(z_mean)[0]\n    dim = tf.keras.backend.int_shape(z_mean)[1]\n    # by default, random_normal has mean=0 and std=1.0\n    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n    return z_mean + tf.keras.backend.exp(0.5 * z_log_var) * epsilon\n    \n    \ndef plot_results(models, data, batch_size=128, model_name=\"vae_mnist\"):\n    \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector\n    # Arguments:\n        models (tuple): encoder and decoder models\n        data (tuple): test data and label\n        batch_size (int): prediction batch size\n        model_name (string): which model is using this function\n    \"\"\"\n\n    encoder, decoder = models\n    x_test, y_test = data\n    os.makedirs(model_name, exist_ok=True)\n\n    filename = os.path.join(model_name, \"vae_mean.png\")\n    # display a 2D plot of the digit classes in the latent space\n    z_mean, _, _ = encoder.predict(x_test,\n                                   batch_size=batch_size)\n    plt.figure(figsize=(12, 10))\n    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n    plt.colorbar()\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.savefig(filename)\n    plt.show()\n\n    filename = os.path.join(model_name, \"digits_over_latent.png\")\n    # display a 30x30 2D manifold of digits\n    n = 30\n    digit_size = 28\n    figure = np.zeros((digit_size * n, digit_size * n))\n    # linearly spaced coordinates corresponding to the 2D plot\n    # of digit classes in the latent space\n    grid_x = np.linspace(-4, 4, n)\n    grid_y = np.linspace(-4, 4, n)[::-1]\n\n    for i, yi in enumerate(grid_y):\n        for j, xi in enumerate(grid_x):\n            z_sample = np.array([[xi, yi]])\n            x_decoded = decoder.predict(z_sample)\n            digit = x_decoded[0].reshape(digit_size, digit_size)\n            figure[i * digit_size: (i + 1) * digit_size,\n                   j * digit_size: (j + 1) * digit_size] = digit\n\n    plt.figure(figsize=(10, 10))\n    start_range = digit_size // 2\n    end_range = n * digit_size + start_range + 1\n    pixel_range = np.arange(start_range, end_range, digit_size)\n    sample_range_x = np.round(grid_x, 1)\n    sample_range_y = np.round(grid_y, 1)\n    plt.xticks(pixel_range, sample_range_x)\n    plt.yticks(pixel_range, sample_range_y)\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.imshow(figure, cmap='Greys_r')\n    plt.savefig(filename)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40a1f16303af6139da25bd6083f02666d48adb42","collapsed":true},"cell_type":"code","source":"(x_train, y_train), (x_test, y_test) = load_data('../input/mnist.npz')\n\nimage_size = x_train.shape[1]\noriginal_dim = image_size * image_size\nx_train = np.reshape(x_train, [-1, original_dim])\nx_test = np.reshape(x_test, [-1, original_dim])\nx_train = x_train.astype('float32') / 255\nx_test = x_test.astype('float32') / 255\n\n# network parameters\ninput_shape = (original_dim, )\nintermediate_dim = 512\nbatch_size = 128\nlatent_dim = 2\nepochs = 50\n\n# VAE model = encoder + decoder\n# build encoder model\ninputs = tf.keras.layers.Input(shape=input_shape, name='encoder_input')\nx = tf.keras.layers.Dense(intermediate_dim, activation='relu')(inputs)\nz_mean = tf.keras.layers.Dense(latent_dim, name='z_mean')(x)\nz_log_var = tf.keras.layers.Dense(latent_dim, name='z_log_var')(x)\n\n# use reparameterization trick to push the sampling out as input\nz = tf.keras.layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n# instantiate encoder model\nencoder = tf.keras.models.Model(inputs, [z_mean, z_log_var, z], name='encoder')\nencoder.summary()\n# tf.keras.utils.plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n\n# build decoder model\nlatent_inputs = tf.keras.layers.Input(shape=(latent_dim,), name='z_sampling')\nx = tf.keras.layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\noutputs = tf.keras.layers.Dense(original_dim, activation='sigmoid')(x)\n\n# instantiate decoder model\ndecoder = tf.keras.models.Model(latent_inputs, outputs, name='decoder')\ndecoder.summary()\n# tf.keras.utils.plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n\n# instantiate VAE model\noutputs = decoder(encoder(inputs)[2])\nvae = tf.keras.models.Model(inputs, outputs, name='vae_mlp')\n\nmodels = (encoder, decoder)\ndata = (x_test, y_test)\n# reconstruction_loss = tf.keras.losses.mse(inputs, outputs)\nreconstruction_loss = tf.keras.losses.binary_crossentropy(inputs, outputs)\n\nreconstruction_loss *= original_dim\n\nkl_loss = 1 + z_log_var - tf.keras.backend.square(z_mean) - tf.keras.backend.exp(z_log_var)\nkl_loss = tf.keras.backend.sum(kl_loss, axis=-1)\nkl_loss *= -0.5\nvae_loss = tf.keras.backend.mean(reconstruction_loss + kl_loss)\nvae.add_loss(vae_loss)\nvae.compile(optimizer='adam')\nvae.summary()\n# tf.keras.utils.plot_model(vae, to_file='vae_mlp.png', show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true,"_uuid":"096400069244884bed9d5a7505bd2ff3dca6ad76"},"cell_type":"code","source":"vae.fit(x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, None))\nvae.save_weights('vae_mlp.mnist.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04890c9e5e5d7aa2c326f34d22aa5a7fd7b694c6","collapsed":true},"cell_type":"code","source":"# Because the VAE is a generative model, we can also use it to generate new digits! Here we will scan the latent plane, sampling latent points at regular intervals and generating the corresponding digit for each of these points. This gives us a visualization of the latent manifold that \"generates\" the MNIST digits.\nplot_results(models, data, batch_size=batch_size, model_name='vae_mlp')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b19cf9d292188c2920d8d5e170b63e38f8ddf747"},"cell_type":"markdown","source":"That's it for Autoecoders. Generative Autoadverserial Networks work much better than Autoencoders. They also generalize really well over a large problem domain. Check out other kernels for some examples of GANs."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}